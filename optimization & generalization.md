## Initialization / Objective Function / Optimization / Generalization
### Optimization
***2016***  
1. Deep Learning without Poor Local Minima, *Kenji Kawaguchi et al, NIPS.* [pdf](https://arxiv.org/pdf/1605.07110.pdf)
1. CONVERGENT LEARNING: DO DIFFERENT NEURAL NETWORKS LEARN THE SAME REPRESENTATIONS?, *Yixuan Li et al, ICLR.* [pdf](https://arxiv.org/pdf/1511.07543.pdf)

***2017***  
1. On Large Batch Training For Deep Learning Generalization Gap and Sharp Minima, *Keskar et al, ICLR.* [pdf](https://arxiv.org/pdf/1609.04836.pdf)
1. Snapshot Ensembles: Train 1, Get M for Free, *Gao Huang et al ICLR.* [pdf](https://arxiv.org/pdf/1704.00109.pdf)  

***2018***
1. How Does Batch Normalization Help Optimization? *Shibani Santurkar et al, NIPS.* [pdf](https://arxiv.org/pdf/1805.11604.pdf)
1. Visualizing the Loss Landscape of Neural Nets, *Hao Li et al, NIPS.* [pdf](https://arxiv.org/pdf/1712.09913.pdf)
1. The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks, *Maxim Berman et al, CVPR.* [pdf](https://arxiv.org/pdf/1705.08790.pdf), [code](https://github.com/bermanmaxim/LovaszSoftmax)

***2019***
1. Decoupled Weight Decay Regularization, *Ilya Loshchilov et al, ICLR.* [pdf](https://arxiv.org/pdf/1711.05101.pdf)
1. Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks, *Guangyong Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/1905.05928.pdf)
1. Understanding the Disharmony between Dropout and Batch Normalization by Variance Shif, *Xiang Li et al, CVPR.* [pdf](https://arxiv.org/pdf/1801.05134.pdf)
1. Averaging Weights Leads to Wider Optima and Better Generalization, *Pavel Izmailov et al, Arxiv.* [pdf](https://arxiv.org/pdf/1803.05407.pdf)
1. Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets, *Rohith Kuditipudi et al, Arxiv.* [pdf](https://arxiv.org/pdf/1906.06247.pdf), [post](http://www.offconvex.org/2019/06/16/modeconnectivity/)
***2021***
1.Do We Need Zero Training Loss After Achieving Zero Training Error? *Takashi Ishida et al, Arxiv.*[pdf](https://arxiv.org/pdf/2002.08709.pdf)
### Generalization
1. Why do deep convolutional networks generalize so poorly to small image transformations? *Aharon Azulay et al, Arxiv* **2018.** [pdf](https://arxiv.org/pdf/1805.12177.pdf)
1. Implicit Regularization in Deep Matrix Factorization, *Sanjeev Arora et al, Arxiv.* [pdf](https://arxiv.org/pdf/1905.13655.pdf)

### Transfer learning
1. Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally, *Zongwei Zhou et al, CVPR* **2017.** [pdf](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf)
