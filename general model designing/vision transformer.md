
### Vision Transformer

#### Survey papers
1. Transformers in Vision: A Survey, *Salman Khan et al, Arxiv,* 2021. [pdf](https://arxiv.org/pdf/2101.01169.pdf)
2. A Survey on Visual Transformer, *Kai Han et al, Arxiv,* 2021. [pdf](https://arxiv.org/pdf/2012.12556.pdf)

#### Model designing
&nbsp;&nbsp;&nbsp;&nbsp;**2020**
1. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, *Alexey Dosovitskiy et al, ICLR.* [pdf](https://arxiv.org/pdf/2010.11929.pdf)

&nbsp;&nbsp;&nbsp;&nbsp;**2021**
1. Transformer in Transformer, *Kai Han et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.00112.pdf%E2%80%8Barxiv.org)
2. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, *Ze Liu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14030.pdf)
3. CvT: Introducing Convolutions to Vision Transformers, *Haiping Wu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.15808.pdf)
4. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, *Chun-Fu Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14899.pdf)
5. Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, *Li Yuan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2101.11986.pdf)
6. Visformer: The Vision-friendly Transformer, *Zhengsu Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2104.12533.pdf)
7. DeepViT: Towards Deeper Vision Transformer, *Daquan Zhou et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.11886.pdf)
8. Evolving Attention with Residual Convolutions, *Yujing Wang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2102.12895.pdf)
9. Towards Robust Vision Transformer, *Xiaofeng Mao et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.07926.pdf)
10. Scaling Local Self-Attention for Parameter Efficient Visual Backbones, *Ashish Vaswani et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.12731.pdf)
11. Scalable Visual Transformers with Hierarchical Pooling, *Zizheng Pan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.10619.pdf)
12. VOLO: Vision Outlooker for Visual Recognition, *Li Yuan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.13112.pdf)
13. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases, *Stephane et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.10697.pdf)
14. Early Convolutions Help Transformers See Better, *Tete Xiao et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.14881v1.pdf)
15. Augmented Shortcuts for Vision Transformers, *Yehui Tang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.15941.pdf)
16. Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer, *Zilong Huang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.03650.pdf)
17. RegionViT: Regional-to-Local Attention for Vision Transformers, *Chun-Fu (Richard) Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.02689.pdf)
18. Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length, *Yulin Wang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.15075.pdf)
19. An Attention Free Transformer, *Shuangfei Zhai et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.14103.pdf)
20. Pay Attention to MLPs, *Hanxiao Liu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.08050.pdf)
21. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, *Yongming Rao et al*. [pdf](https://arxiv.org/pdf/2106.02034.pdf)
22. CoAtNet: Marrying Convolution and Attention for All Data Sizes, *Zihang Dai et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.04803.pdf)
23. Dual-stream Network for Visual Recognition, *Mingyuan Mao et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.14734.pdf)
24. X-volution: On the Unification of Convolution and Self-attention, *Xuanhong Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.02253.pdf)
25. Transformer in Convolutional Neural Networks, *Yun Liu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.03180.pdf)
26. LocalViT: Bringing Locality to Vision Transformers, *Yawei Li et al, Arxiv.* [pdf](https://arxiv.org/pdf/2104.05707.pdf)
27. Aggregating Nested Transformers, *Zizhao Zhang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.12723.pdf)

#### MISC
1. Understanding Robustness of Transformers for Image Classification, *Srinadh Bhojanapalli et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14586.pdf)
2. Intriguing Properties of Vision Transformers, *Muzammal Naseer et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.10497v1.pdf)
3. Emerging Properties in Self-Supervised Vision Transformers, *Mathilde Caron et al, Arxiv.* [pdf](https://arxiv.org/pdf/2104.14294.pdf)
4. Are Pre-trained Convolutions Better than Pre-trained Transformers? *Yi Tay et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.03322.pdf)
5. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, *Andreas Steiner et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.10270.pdf)
6. Scaling Vision Transformers, *Xiaohua Zhai et al, Arxiv*. [pdf](https://arxiv.org/pdf/2106.04560.pdf)
7. When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations,*Xiangning Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.01548.pdf)


