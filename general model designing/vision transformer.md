
### Vision Transformer

#### Survey papers
1. Transformers in Vision: A Survey, *Salman Khan et al, Arxiv,* 2021. [pdf](https://arxiv.org/pdf/2101.01169.pdf)
2. A Survey on Visual Transformer, *Kai Han et al, Arxiv,* 2021. [pdf](https://arxiv.org/pdf/2012.12556.pdf)

#### Model designing
&nbsp;&nbsp;&nbsp;&nbsp;**2020**
1. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, *Alexey Dosovitskiy et al, ICLR.* [pdf](https://arxiv.org/pdf/2010.11929.pdf)

&nbsp;&nbsp;&nbsp;&nbsp;**2021**
1. Transformer in Transformer, *Kai Han et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.00112.pdf%E2%80%8Barxiv.org)
2. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, *Ze Liu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14030.pdf)
3. CvT: Introducing Convolutions to Vision Transformers, *Haiping Wu et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.15808.pdf)
4. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, *Chun-Fu Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14899.pdf)
5. Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, *Li Yuan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2101.11986.pdf)
6. Visformer: The Vision-friendly Transformer, *Zhengsu Chen et al, Arxiv.* [pdf](https://arxiv.org/pdf/2104.12533.pdf)
7. DeepViT: Towards Deeper Vision Transformer, *Daquan Zhou et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.11886.pdf)
8. Evolving Attention with Residual Convolutions, *Yujing Wang et al, Arxiv.* [pdf](https://arxiv.org/pdf/2102.12895.pdf)
9. Towards Robust Vision Transformer, *Xiaofeng Mao et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.07926.pdf)
10. Scaling Local Self-Attention for Parameter Efficient Visual Backbones, *Ashish Vaswani et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.12731.pdf)
11. Scalable Visual Transformers with Hierarchical Pooling, *Zizheng Pan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.10619.pdf)
12. VOLO: Vision Outlooker for Visual Recognition, *Li Yuan et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.13112.pdf)
13. ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases, *Stephane et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.10697.pdf)
14. Early Convolutions Help Transformers See Better, *Tete Xiao et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.14881v1.pdf)


#### MISC
1. Understanding Robustness of Transformers for Image Classification, *Srinadh Bhojanapalli et al, Arxiv.* [pdf](https://arxiv.org/pdf/2103.14586.pdf)
2. Intriguing Properties of Vision Transformers, *Muzammal Naseer et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.10497v1.pdf)
3. Emerging Properties in Self-Supervised Vision Transformers, *Mathilde Caron et al, Arxiv.* [pdf](https://arxiv.org/pdf/2104.14294.pdf)
4. Are Pre-trained Convolutions Better than Pre-trained Transformers? *Yi Tay et al, Arxiv.* [pdf](https://arxiv.org/pdf/2105.03322.pdf)
5. How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, *Andreas Steiner et al, Arxiv.* [pdf](https://arxiv.org/pdf/2106.10270.pdf)



